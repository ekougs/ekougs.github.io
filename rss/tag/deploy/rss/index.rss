<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="//purl.org/dc/elements/1.1/" xmlns:content="//purl.org/rss/1.0/modules/content/" xmlns:atom="//www.w3.org/2005/Atom" version="2.0" xmlns:media="//search.yahoo.com/mrss/"><channel><title><![CDATA[deploy - The bytes bait]]></title><description><![CDATA[My stories about development and deployment. Written and shared with humor and passion.]]></description><link>//localhost:8008/</link><image><url>//localhost:8008/favicon.png</url><title>deploy - The bytes bait</title><link>//localhost:8008/</link></image><generator>Ghost 2.31</generator><lastBuildDate>Wed, 03 Jun 2020 21:02:33 GMT</lastBuildDate><atom:link href="//localhost:8008/tag/deploy/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Lessons learned with Docker]]></title><description><![CDATA[<p>For the last projects I built, I decided to take a shot at Docker for several reasons that seemed good to me.<br>I wanted to go live fast with a production environment easily reproducible on my dev laptop. I was alone, scripting the installation and configuration of all the layers</p>]]></description><link>//localhost:8008/lessons-learned-with-docker/</link><guid isPermaLink="false">5ec6a3f0a5d121006e416938</guid><category><![CDATA[ci]]></category><category><![CDATA[docker]]></category><category><![CDATA[deploy]]></category><dc:creator><![CDATA[ekougs]]></dc:creator><pubDate>Tue, 21 Feb 2017 22:34:00 GMT</pubDate><content:encoded><![CDATA[<p>For the last projects I built, I decided to take a shot at Docker for several reasons that seemed good to me.<br>I wanted to go live fast with a production environment easily reproducible on my dev laptop. I was alone, scripting the installation and configuration of all the layers (application, proxy, cache, dB) was prohibitive because I would spend too much time on it. Docker seemed really attractive as each layer is in a separate container.<br>Plus, deploying today a « dockerized » application is cheap thanks to providers like Digital Ocean (I am a customer and fan).<br>Usually, I would buy a book or follow thoroughly recommended tutorials before using the technology. The main drawback is that it is often time consuming and you don’t really get immediately usable skills. So this time I decided to directly get my hands dirty. As expected this was an interesting journey and these are the lessons I learnt along the way.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h1 id="layer-caching-is-awesome">Layer caching is awesome</h1><p>This is the first thing that you learn, especially when you have a really low bandwidth.<br>At first, when building my docker images, I would naively write the following Dockerfile (I voluntarily truncated it to highlight the problem).</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
ADD . .
RUN pip install -r requirements.txt
...</code></pre><figcaption>Dockerfile not using layer cache for dependencies</figcaption></figure><!--kg-card-end: code--><p>But everytime I would build the image, it would take way too long to finish. Especially when your connection is 2 mbps at best, I currently live in Senegal and realized I was a spoiled kid in France with a fiber connection and at least 50mbps. You quickly run out of patience and have to do better.</p><p>Fortunately, this was extremely easy to solve, thanks again StackOverflow. I quickly learnt that a better version of what I tried to do was the following.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
ADD requirements.txt .
RUN pip install -r requirements.txt
ADD . .
...</code></pre><figcaption>Dockerfile using layer cache for dependencies</figcaption></figure><!--kg-card-end: code--><p>Docker creates a layer for each instruction, except the metadata one like <code>MAINTAINER</code> for example, and caches it. If the conditions leading to the creation of the layer are unchanged, it can use the cached version. In this particular case, if requirements.txt is unchanged, the 2nd layer linked to the first ADD instruction is retrieved in the cache. Thus the condition to create the 3rd layer have not changed as the filesystem is changed by the first ADD, so it will also hit the cache if requirements.txt is not changed. So no more costly downloads for each change. It will only occurs when it has to i.e. when my requirements/dependencies change.</p><p><strong><strong>Takeaway</strong></strong>: in your dockerfile, always write the part that are less likely to change first.</p><h1 id="add-and-copy"><strong>ADD and COPY</strong></h1><p>Then I noticed that doing what you saw above was not really wise. I was copying every source to build the project in the Docker image. But that implied also copying tests and static assets that were not needed. So I decided to build externally the project and copy it then in the image so that I only get what I need into it. I would do the following in my Dockerfile.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">ADD myproject.tar.gz .
RUN pip install -q myproject.tar.gz
...</code></pre><figcaption>Dockerfile - auto-uncompress with ADD</figcaption></figure><!--kg-card-end: code--><p>But the second instruction would not work. I was a bit perplex as it was working on the host OS (my laptop) but not in the image. After some debugging, typically I built the image without the pip install part and check what was inside it, I found that the archive was in fact uncompressed. Then, I found out that ADD has an auto uncompress feature for some supported compression formats unlike its sibling COPY.</p><p><strong><strong>Takeaway</strong></strong>: avoid using ADD if you don’t want the auto-uncompress feature.</p><p>I, unfortunately, encountered a major issue with those two. I did not like the fact that most images use root user as default. So I changed it in my images before doing anything else with the <code>USER</code> instruction. You would then expect that the other instructions including <code>ADD/COPY</code> would take the user into account. But this was not the case. It was really problematic because I would change the user in the image, copy some static assets in a nginx image and the container would not be able to serve them because the user has no right to read the files. This one took me some hours before finding the cause as I was really not expecting this behavior. The solution is that you had to manually change the owner on the copied files in the Dockerfile. Not cool, especially as you would have to write a <code>RUN</code>, so at least create another layer in your image. It took years but it has finally been solved recently and you can now use the <code>--chown</code> flag with ADD and COPY.</p><p><a href="dockerfile:%20ADD%20does%20not%20honor%20USER:%20files%20always%20owned%20by%20root%20%C2%B7%20Issue%20#6119%20%C2%B7%20moby/moby%20Hi,%20consider%20this%20Dockerfile:%20FROM%20ubuntu%20RUN%20adduser%20foo%20USER%20foo%20ADD%20.%20/foo%20/foo%20in%20the%20container%20will%20be%20owned%20by%E2%80%A6%20github.com">https://github.com/moby/moby/issues/6119?source=post_page-------------------------—</a></p><h2 id="build-in-the-appropriate-environment">Build in the appropriate environment</h2><p>Building a project externally before copying it in the image is not the ideal way to proceed. I would build the artifact to deploy in MacOS or whatever OS my CI was based on and then copy it in a container based on Ubuntu. It worked well in this case because the artifact and the build are cross-platform. But what if it wasn’t ? I would spend a hell lot of time to make it work. At the time, I was not aware of the multi-stage build feature in Docker. This is really convenient when you want reproducible builds and build minimal images. In the first stage, you build the artifact, and in the second step, you copy the built artifact in a minimal base image, et voilà. Concretely this is what it could look like.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6 as build
COPY requirements.txt .
RUN pip install -q -r requirements.txt
COPY . .  
RUN python setup.py -q sdist
...
FROM python:3.6-slim-jessie
COPY --from=build /usr/local/lib/python3.6/site-packages/ /usr/local/lib/python3.6/site-packages/
COPY --from=build /usr/local/bin /usr/local/bin
...</code></pre><figcaption>Dockerfile - multilevel build</figcaption></figure><!--kg-card-end: code--><p>In the first stage, we create the artifact. We can use an “heavy” image, it does not matter as it will not be the base of the final image. We can use a container with a full development environment. Don’t worry, the created layers in the build steps won’t even be part of the final image. The final build step is our concrete image. We named the first build step as build. We can then use it the last one to copy all the artifacts we need. The base of the final image will be <code>python:3.6-slim-jessie</code> which is lighter than the build image. My first images weighted 300MB, I reduced it to 100MB with this feature. It is even more impressive with projects where the final artifact is a binary, like Go projects, that can run in a slim alpine container that weights only tens of MB including the binary.</p><p><strong><strong>Takeaway</strong></strong>: Run the build in a container to make it reproducible and get slim target image.</p><h1 id="go-easy-on-layers">Go easy on layers</h1><p>Earlier, we saw the superpower of layer caching. It can be tempting to separate each instruction in its own layer. Because, you know, it may be easier to read and change later. It could look like the following.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
...
RUN apt-get update
RUN apt-get install wget
...</code></pre><figcaption>Dockerfile with 2 many layers</figcaption></figure><!--kg-card-end: code--><p>But it is not a great idea. Creating a layer comes with its own overhead. Thus doing this will considerably increase the size of the resulting image.</p><p>So the usual practice is to do that when we test the Dockerfile locally. If we have several instructions to run, we are confident about some of them but less about others, we write the instructions for the former first and then we add the latter. So if the build fails, it will only run the instructions we fixed and not all of them. And once we are confident, we can group the instructions in a single one.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
...
RUN apt-get update &amp;&amp; apt-get install wget
...</code></pre><figcaption>Dockerfile - Merging 2 commands to avoid several layers</figcaption></figure><!--kg-card-end: code--><p>This is what you will usually see in the official libraries repositories like the following.</p><p><a href="https://github.com/docker-library/python/blob/master/3.6/jessie/Dockerfile?source=post_page---------------------------">https://github.com/docker-library/python/blob/master/3.6/jessie/Dockerfile?source=post_page---------------------------</a></p><p><strong><strong>Takeaway</strong></strong>: Do not create unnecessary layers to avoid getting bloated images.</p><h1 id="build-working-images">Build working images</h1><p>It is important to only build images if you are confident that they will work. Especially for the images where you deploy custom code like the Python application example I introduced above.</p><p>Usually, you get this confidence by writing a tests suite and automatically running it before building any artifact, the very concept of CI. You run unit, sometimes integration and E2E, tests before building a JAR or any other artifacts, so you should do the same with your Docker images. This is a great match for our new favorite, the multistage build. We stated above that it gives us the opportunity to get a reproducible environment. We are going to leverage it to run our tests before even building the different assets.</p><p>So how does it translate to our previous example ? Just how I think you would do it given the explanation above.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6 as build
COPY requirements.txt .
RUN pip install -q -r requirements.txt
COPY . .
RUN flake8 &amp;&amp; pytest &amp;&amp; python setup.py -q sdist
...
FROM python:3.6-slim-jessie
COPY --from=build /usr/local/lib/python3.6/site-packages/ /usr/local/lib/python3.6/site-packages/
COPY --from=build /usr/local/bin /usr/local/bin
...</code></pre><figcaption>Dockerfile - Test in the first level before actual image in the final level</figcaption></figure><!--kg-card-end: code--><p>We run the tests just before building the egg. If our tests fail, the build fails too independently of the platform running Docker. You get a report, you can fix before relaunching the artifact building.</p><p><strong><strong>Takeaway</strong></strong>: Build the safer artifacts possible by testing in the build phase</p><h1 id="simplify-ci-and-deployments">Simplify CI and deployments</h1><p>At first, I was storing many types of artifacts. This implied managing multiple credentials over multiple platforms (because I was working with some free online services, no money :) ), setting up an upload type per artifact (docker registry, pypi), building then sending each artifact after each successful CI build. Anyway, this can only be useful if both artifacts are used. This could be the case if I was using several types of deployments (container in development environment, VMs in production) or I was developing an application for a client and agreed on a specific artifact to deliver for example.</p><p>But as I owned the application and planned to use Docker from my laptop to my production environment, it simply did not make sense for me to manage several types of artifacts. And it did sting me several times when I made mistakes and wanted to delete the built versions. Finally, I decided that the only artifact that matters is the Docker image. I am now using a Docker registry as my single artifact repository as this is the only thing I need to deploy whatever environments I deploy on. One artifact, one registry, for simpler CI builds and deployments.</p><p><strong><strong>Takeaway</strong></strong>: This one is rather general but do not build unnecessary artifacts on your road to production, keep your deployments simple.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h1 id="the-next-mis-steps">The next (mis)steps</h1><p>All in all, I am really glad I learnt this way. Docker was in fact a right tool to do that. You can learn it as you go and Docker always has something to teach you along the way. I made mistakes, I learnt from them and there is still a lot to learn before being sufficiently proficient. How marvelous.</p><p>The next steps I want to explore: improve security of my containers, deploy a simple and informative monitoring for my containers, play more with container orchestration, see what LinuxKit can bring to the table.</p><p>I will be glad to read your tips in the comments. Here are resources I used:</p><p>Official Docker documentation</p><p><a href="https://docs.docker.com/?source=post_page---------------------------">https://docs.docker.com/?source=post_page---------------------------</a></p><p>Docker training</p><p><a href="https://container.training/?source=post_page---------------------------">//container.training/?source=post_page---------------------------</a></p><p><a href="https://training.play-with-docker.com/ops-landing/?source=post_page---------------------------">//training.play-with-docker.com/ops-landing/?source=post_page---------------------------</a></p><p>Play with Docker, to use Docker without installing it</p><p><a href="https://labs.play-with-docker.com/?source=post_page---------------------------">https://labs.play-with-docker.com/?source=post_page---------------------------</a></p>]]></content:encoded></item></channel></rss>