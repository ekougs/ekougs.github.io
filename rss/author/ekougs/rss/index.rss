<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="//purl.org/dc/elements/1.1/" xmlns:content="//purl.org/rss/1.0/modules/content/" xmlns:atom="//www.w3.org/2005/Atom" version="2.0" xmlns:media="//search.yahoo.com/mrss/"><channel><title><![CDATA[ekougs - The bytes bait]]></title><description><![CDATA[My stories about development and deployment. Written and shared with humor and passion.]]></description><link>//localhost:8008/</link><image><url>//localhost:8008/favicon.png</url><title>ekougs - The bytes bait</title><link>//localhost:8008/</link></image><generator>Ghost 2.31</generator><lastBuildDate>Sat, 30 May 2020 21:13:56 GMT</lastBuildDate><atom:link href="//localhost:8008/author/ekougs/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Test your Django SysLog configuration with Docker]]></title><description><![CDATA[<p>Not long ago, I was finalizing the deployment of the MVP of an internal tool.<br>Our standard containerized environment use Syslog to dispatch logs.<br>The aforementioned tool is written in Django that fortunately has rfc5424-logging-handler library for handling log communication to SysLog.<br>But still I needed to test that my</p>]]></description><link>//localhost:8008/test-your-syslog-configuration-with-docker/</link><guid isPermaLink="false">5ed2b465b4e421006eabab68</guid><category><![CDATA[docker]]></category><category><![CDATA[syslog]]></category><category><![CDATA[linux]]></category><category><![CDATA[python]]></category><category><![CDATA[django]]></category><dc:creator><![CDATA[ekougs]]></dc:creator><pubDate>Sat, 30 May 2020 21:04:07 GMT</pubDate><content:encoded><![CDATA[<p>Not long ago, I was finalizing the deployment of the MVP of an internal tool.<br>Our standard containerized environment use Syslog to dispatch logs.<br>The aforementioned tool is written in Django that fortunately has rfc5424-logging-handler library for handling log communication to SysLog.<br>But still I needed to test that my configuration works before any actual deployment.<br>That is where Docker came to the rescue</p><h2 id="django-configuration">Django configuration</h2><p>Below the Django configuration I used</p><!--kg-card-begin: code--><pre><code class="language-python">LOGGING["handlers"]["syslog"] = {
    "level": LOG_LEVEL,
    "formatter": "mesos",
    "class": "rfc5424logging.handler.Rfc5424SysLogHandler",
    "appname": app_name,
    "facility": LOG_LOCAL5,
}
LOGGING["root"]["handlers"].append("syslog")</code></pre><!--kg-card-end: code--><p>It is rather trivial I declare the Syslog handler I want to use and the facility that will be used. Not that it uses the UDP protocol by default.</p><h2 id="the-docker-image">The docker image</h2><p>I did not want to install Syslog or change the configuration locally.<br>I try to isolate as much as I can the changes because it is easier to switch environments between projects and spot problems when you introduce changes.<br></p><p>This is why I created the following Docker image to test my setup</p><!--kg-card-begin: code--><pre><code class="language-Dockerfile">FROM python:3.6-buster

RUN apt-get update \
    &amp;&amp; apt-get install -y --no-install-recommends \
        rsyslog vim less \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

RUN echo '\nlocal5.*              /var/log/internal_tool.log' &gt;&gt; /etc/rsyslog.conf

WORKDIR /usr/src/app
COPY requirements ./requirements
RUN pip install -r requirements/base.txt -r requirements/local.txt
COPY . .

EXPOSE 8000
CMD ["python", "manage.py", "runserver", "0.0.0.0:8000"]</code></pre><!--kg-card-end: code--><p>It installs syslog, redirects local5 facility to a local file, install our app and runs it.</p><p>So then we can build, <code>docker image build...</code>, and run the container, <code>docker container exec...</code>.<br>As rsyslog is not listening UDP:514 by default, like the default log handler configuration above expects, we uncomment the following lines in <code>/etc/rsyslog.conf</code>.</p><!--kg-card-begin: code--><pre><code>#module(load="imudp")
#input(type="imudp" port="514")</code></pre><!--kg-card-end: code--><p>Note that we could also have prepared a configuration file and load it directly in the image.<br>Then restart the rsylog daemon <code>/usr/sbin/rsyslogd -n -iNONE&amp;</code></p><h2 id="finally">Finally</h2><p>You can test that your configuration is OK with <code>logger -p local5.error "Troubleshooting test"</code>.<br>We expect it to create or update <code>/var/log/internal_tool.log</code> file with the phrase "Troubleshooting test" in it.</p>]]></content:encoded></item><item><title><![CDATA[Quarkus + Auth0 , the missing guide]]></title><description><![CDATA[<h2 id="the-situation">The situation</h2><p>I am currently playing with <a href="https://twitter.com/QuarkusIO">@QuarkusIO</a> (for the native app promise) and <a href="https://twitter.com/kotlin">@kotlin</a> and was looking to integrate it with an IAM using JWT.<br>And because I am generous and don’t want you to spend as much time on this, may the following be of some help.</p>]]></description><link>//localhost:8008/quarkus-auth0-the-missing-guide/</link><guid isPermaLink="false">5ec6a26da5d121006e41691b</guid><category><![CDATA[iam]]></category><category><![CDATA[jwt]]></category><category><![CDATA[quarkus]]></category><category><![CDATA[auth0]]></category><category><![CDATA[dev]]></category><dc:creator><![CDATA[ekougs]]></dc:creator><pubDate>Wed, 07 Aug 2019 19:46:00 GMT</pubDate><content:encoded><![CDATA[<h2 id="the-situation">The situation</h2><p>I am currently playing with <a href="https://twitter.com/QuarkusIO">@QuarkusIO</a> (for the native app promise) and <a href="https://twitter.com/kotlin">@kotlin</a> and was looking to integrate it with an IAM using JWT.<br>And because I am generous and don’t want you to spend as much time on this, may the following be of some help.<br>So I chose to use <a href="https://twitter.com/auth0">@auth0</a> for the popular social networks integrations, the great support and tons of great guides, and the free offer for a solid authentication and authorization service.<br>Unfortunately for the lazy me, I did not find a straightforward guide. There are great guides for Spring and native backend but obviously nothing that could help me quickly.<br>If you are only interested by the solution, just click <a>here</a>.</p><h2 id="and-excitement-almost-became-panic">And excitement almost became panic</h2><p>« Yeah, great, time to start a little sidequest ».<br>But not too long because, hey, I still have to get back to my primary purpose.<br><strong>So first I created a</strong> <a href="https://auth0.com/docs/dashboard/guides/applications/register-app-native">native application</a> <strong>on Auth0 to support my solution.</strong></p><p>Then it was time to make it work with my generated Quarkus backend. Quarkus’ documentation is mostly easy to use. But when it came to the authentication/security settings, I was kind of lost at first.<br>So lost, that at a point, I considered writing my own custom filter to do this.<br>But I (almost) immediately felt that it was overkill and that there should be something simpler out there.</p><p>And then I chilled down, remembered that I was tired (never work tired, just never), and also thought that the people behind the framework were too smart not to provide easy ways to validate a token.</p><h2 id="enough-blabbering-the-facts-">Enough blabbering, the facts!</h2><p>At this point, I already spent 3 to 4 x 3h looking for a viable solution.<br>I then encountered my first pain point.</p><h3 id="debugging-ability-is-a-life-savior">Debugging ability is a life savior</h3><p>This is  when after a few searches and tricks not worthy to mention here, <strong>I found this Github issue</strong> <a href="https://github.com/quarkusio/quarkus/issues/1163">smallrye-jwt not working in native mode · Issue #1163 · quarkusio/quarkus · GitHub</a> that retrospectively was the first step toward resolution.<br>After reading the changes and analyzing what I should do, I decided that I would debug a little to understand what was wrong with my settings.<br>Usually, debugging is as easy as clicking a button on my favorite IDE, Intellij.<br>It didn’t work and I still don’t know why, because I didn’t take time to dig deeper.<br>But <strong>I found an alternative, run the</strong> <a href="https://quarkus.io/guides/getting-started-guide#running-the-application">Quarkus application</a><strong>, then</strong> <a href="https://www.jetbrains.com/help/idea/attaching-to-local-process.html">attach a debugger</a> <strong>to it</strong>. A little tedious but nothing unsurmountable.</p><h3 id="so-what-was-wrong">So what was wrong</h3><p>This is really where you should pay attention :smiley:<br>I first <strong>amended my pom to get the most recent available version for Quarkus</strong> (0.19.1 at the time).<br>As I debugged the applications and added enough <a href="https://quarkus.io/guides/logging-guide#logging-categories">logs for the targeted category</a> (<code>io.quarkus.elytron</code>, <code>io.smallrye.jwt</code>), I found the following to be able to read the JWT:</p><ul><li>You need the public key of the certificate used to sign it. You can find the certificate in the settings of your Auth0 application. Just scroll until you see the <strong>Show Advanced Settings</strong> link. Click on it, access the <strong>Certificates</strong> tab, download it with the <strong>Download certificate</strong> button. I spent too much time using the certificate instead of its public key. Be smarter. You can use this command to extract it <code>openssl x509 -pubkey -noout -in cert.pem  &gt; pubkey.pem</code>.</li><li>The <code>mp.jwt.verify.issuer</code> is the <strong>Domain</strong> of your Auth0 application.</li></ul><h2 id="auth0-integration-guide">Auth0 integration guide</h2><p>The relevant parts of the solution</p><ul><li>Create a <a href="https://auth0.com/docs/dashboard/guides/applications/register-app-native">native application</a> on Auth0</li><li>Go to the Auth0 application settings, scroll until you see the <strong>Show Advanced Settings</strong> link. Click on it, access the <strong>Certificates</strong> tab, download it with the <strong>Download certificate</strong> button.</li><li>Don’t forget to extract the public key to the appropriate location  <code>openssl x509 -pubkey -noout -in cert.pem  &gt; pubkey.pem</code>.</li></ul><p>Extract of the <code>pom.xml</code></p><!--kg-card-begin: code--><pre><code>&lt;properties&gt;
	&lt;quarkus.version&gt;0.19.1&lt;/quarkus.version&gt;
	...
&lt;/properties&gt;
</code></pre><!--kg-card-end: code--><p>Extract of the security settings of the  <code>application.properties</code> file</p><!--kg-card-begin: code--><pre><code>mp.jwt.verify.publickey.location=META-INF/resources/pubkey.pem
mp.jwt.verify.issuer=https://dev-******.eu.auth0.com/

quarkus.smallrye-jwt.auth-mechanism=MP-JWT
quarkus.smallrye-jwt.enabled=true
</code></pre><!--kg-card-end: code--><p>The REST resource used to test</p><!--kg-card-begin: code--><pre><code>@Path("/test")
@RequestScoped
class TestJWTService {
    @GET
    @PermitAll
    @Produces(MediaType.TEXT_PLAIN)
    fun hello(@Context ctx: SecurityContext): String {
        val caller = ctx.userPrincipal
        val name = if (caller == null) "anonymous" else caller.name
        val isSecure = ctx.isSecure
        val authScheme = ctx.authenticationScheme
        return "hello + $name, isSecure: $isSecure, authScheme: $authScheme"
    }
}
</code></pre><!--kg-card-end: code--><p>Get the token</p><!--kg-card-begin: code--><pre><code>curl --request POST \
  --url https://dev-bhmhh10f.eu.auth0.com/oauth/token \
  --header 'content-type: application/json' \
  --data '{"client_id":"your_client_id","client_secret":"your_client_secret","audience":"the_audience","grant_type":"client_credentials"}'
</code></pre><!--kg-card-end: code--><p>Test the token you just got</p><!--kg-card-begin: code--><pre><code>curl --request GET --url //localhost:8080/bags --header 'authorization: Bearer the_token_you_just_got'
</code></pre><!--kg-card-end: code--><p><a href="https://quarkus.io/guides/jwt-guide">Original documentation</a></p>]]></content:encoded></item><item><title><![CDATA[Lessons learned with Docker]]></title><description><![CDATA[<p>For the last projects I built, I decided to take a shot at Docker for several reasons that seemed good to me.<br>I wanted to go live fast with a production environment easily reproducible on my dev laptop. I was alone, scripting the installation and configuration of all the layers</p>]]></description><link>//localhost:8008/lessons-learned-with-docker/</link><guid isPermaLink="false">5ec6a3f0a5d121006e416938</guid><category><![CDATA[ci]]></category><category><![CDATA[docker]]></category><category><![CDATA[deploy]]></category><dc:creator><![CDATA[ekougs]]></dc:creator><pubDate>Tue, 21 Feb 2017 22:34:00 GMT</pubDate><content:encoded><![CDATA[<p>For the last projects I built, I decided to take a shot at Docker for several reasons that seemed good to me.<br>I wanted to go live fast with a production environment easily reproducible on my dev laptop. I was alone, scripting the installation and configuration of all the layers (application, proxy, cache, dB) was prohibitive because I would spend too much time on it. Docker seemed really attractive as each layer is in a separate container.<br>Plus, deploying today a « dockerized » application is cheap thanks to providers like Digital Ocean (I am a customer and fan).<br>Usually, I would buy a book or follow thoroughly recommended tutorials before using the technology. The main drawback is that it is often time consuming and you don’t really get immediately usable skills. So this time I decided to directly get my hands dirty. As expected this was an interesting journey and these are the lessons I learnt along the way.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h1 id="layer-caching-is-awesome">Layer caching is awesome</h1><p>This is the first thing that you learn, especially when you have a really low bandwidth.<br>At first, when building my docker images, I would naively write the following Dockerfile (I voluntarily truncated it to highlight the problem).</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
ADD . .
RUN pip install -r requirements.txt
...</code></pre><figcaption>Dockerfile not using layer cache for dependencies</figcaption></figure><!--kg-card-end: code--><p>But everytime I would build the image, it would take way too long to finish. Especially when your connection is 2 mbps at best, I currently live in Senegal and realized I was a spoiled kid in France with a fiber connection and at least 50mbps. You quickly run out of patience and have to do better.</p><p>Fortunately, this was extremely easy to solve, thanks again StackOverflow. I quickly learnt that a better version of what I tried to do was the following.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
ADD requirements.txt .
RUN pip install -r requirements.txt
ADD . .
...</code></pre><figcaption>Dockerfile using layer cache for dependencies</figcaption></figure><!--kg-card-end: code--><p>Docker creates a layer for each instruction, except the metadata one like <code>MAINTAINER</code> for example, and caches it. If the conditions leading to the creation of the layer are unchanged, it can use the cached version. In this particular case, if requirements.txt is unchanged, the 2nd layer linked to the first ADD instruction is retrieved in the cache. Thus the condition to create the 3rd layer have not changed as the filesystem is changed by the first ADD, so it will also hit the cache if requirements.txt is not changed. So no more costly downloads for each change. It will only occurs when it has to i.e. when my requirements/dependencies change.</p><p><strong><strong>Takeaway</strong></strong>: in your dockerfile, always write the part that are less likely to change first.</p><h1 id="add-and-copy"><strong>ADD and COPY</strong></h1><p>Then I noticed that doing what you saw above was not really wise. I was copying every source to build the project in the Docker image. But that implied also copying tests and static assets that were not needed. So I decided to build externally the project and copy it then in the image so that I only get what I need into it. I would do the following in my Dockerfile.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">ADD myproject.tar.gz .
RUN pip install -q myproject.tar.gz
...</code></pre><figcaption>Dockerfile - auto-uncompress with ADD</figcaption></figure><!--kg-card-end: code--><p>But the second instruction would not work. I was a bit perplex as it was working on the host OS (my laptop) but not in the image. After some debugging, typically I built the image without the pip install part and check what was inside it, I found that the archive was in fact uncompressed. Then, I found out that ADD has an auto uncompress feature for some supported compression formats unlike its sibling COPY.</p><p><strong><strong>Takeaway</strong></strong>: avoid using ADD if you don’t want the auto-uncompress feature.</p><p>I, unfortunately, encountered a major issue with those two. I did not like the fact that most images use root user as default. So I changed it in my images before doing anything else with the <code>USER</code> instruction. You would then expect that the other instructions including <code>ADD/COPY</code> would take the user into account. But this was not the case. It was really problematic because I would change the user in the image, copy some static assets in a nginx image and the container would not be able to serve them because the user has no right to read the files. This one took me some hours before finding the cause as I was really not expecting this behavior. The solution is that you had to manually change the owner on the copied files in the Dockerfile. Not cool, especially as you would have to write a <code>RUN</code>, so at least create another layer in your image. It took years but it has finally been solved recently and you can now use the <code>--chown</code> flag with ADD and COPY.</p><p><a href="dockerfile:%20ADD%20does%20not%20honor%20USER:%20files%20always%20owned%20by%20root%20%C2%B7%20Issue%20#6119%20%C2%B7%20moby/moby%20Hi,%20consider%20this%20Dockerfile:%20FROM%20ubuntu%20RUN%20adduser%20foo%20USER%20foo%20ADD%20.%20/foo%20/foo%20in%20the%20container%20will%20be%20owned%20by%E2%80%A6%20github.com">https://github.com/moby/moby/issues/6119?source=post_page-------------------------—</a></p><h2 id="build-in-the-appropriate-environment">Build in the appropriate environment</h2><p>Building a project externally before copying it in the image is not the ideal way to proceed. I would build the artifact to deploy in MacOS or whatever OS my CI was based on and then copy it in a container based on Ubuntu. It worked well in this case because the artifact and the build are cross-platform. But what if it wasn’t ? I would spend a hell lot of time to make it work. At the time, I was not aware of the multi-stage build feature in Docker. This is really convenient when you want reproducible builds and build minimal images. In the first stage, you build the artifact, and in the second step, you copy the built artifact in a minimal base image, et voilà. Concretely this is what it could look like.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6 as build
COPY requirements.txt .
RUN pip install -q -r requirements.txt
COPY . .  
RUN python setup.py -q sdist
...
FROM python:3.6-slim-jessie
COPY --from=build /usr/local/lib/python3.6/site-packages/ /usr/local/lib/python3.6/site-packages/
COPY --from=build /usr/local/bin /usr/local/bin
...</code></pre><figcaption>Dockerfile - multilevel build</figcaption></figure><!--kg-card-end: code--><p>In the first stage, we create the artifact. We can use an “heavy” image, it does not matter as it will not be the base of the final image. We can use a container with a full development environment. Don’t worry, the created layers in the build steps won’t even be part of the final image. The final build step is our concrete image. We named the first build step as build. We can then use it the last one to copy all the artifacts we need. The base of the final image will be <code>python:3.6-slim-jessie</code> which is lighter than the build image. My first images weighted 300MB, I reduced it to 100MB with this feature. It is even more impressive with projects where the final artifact is a binary, like Go projects, that can run in a slim alpine container that weights only tens of MB including the binary.</p><p><strong><strong>Takeaway</strong></strong>: Run the build in a container to make it reproducible and get slim target image.</p><h1 id="go-easy-on-layers">Go easy on layers</h1><p>Earlier, we saw the superpower of layer caching. It can be tempting to separate each instruction in its own layer. Because, you know, it may be easier to read and change later. It could look like the following.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
...
RUN apt-get update
RUN apt-get install wget
...</code></pre><figcaption>Dockerfile with 2 many layers</figcaption></figure><!--kg-card-end: code--><p>But it is not a great idea. Creating a layer comes with its own overhead. Thus doing this will considerably increase the size of the resulting image.</p><p>So the usual practice is to do that when we test the Dockerfile locally. If we have several instructions to run, we are confident about some of them but less about others, we write the instructions for the former first and then we add the latter. So if the build fails, it will only run the instructions we fixed and not all of them. And once we are confident, we can group the instructions in a single one.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6
...
RUN apt-get update &amp;&amp; apt-get install wget
...</code></pre><figcaption>Dockerfile - Merging 2 commands to avoid several layers</figcaption></figure><!--kg-card-end: code--><p>This is what you will usually see in the official libraries repositories like the following.</p><p><a href="https://github.com/docker-library/python/blob/master/3.6/jessie/Dockerfile?source=post_page---------------------------">https://github.com/docker-library/python/blob/master/3.6/jessie/Dockerfile?source=post_page---------------------------</a></p><p><strong><strong>Takeaway</strong></strong>: Do not create unnecessary layers to avoid getting bloated images.</p><h1 id="build-working-images">Build working images</h1><p>It is important to only build images if you are confident that they will work. Especially for the images where you deploy custom code like the Python application example I introduced above.</p><p>Usually, you get this confidence by writing a tests suite and automatically running it before building any artifact, the very concept of CI. You run unit, sometimes integration and E2E, tests before building a JAR or any other artifacts, so you should do the same with your Docker images. This is a great match for our new favorite, the multistage build. We stated above that it gives us the opportunity to get a reproducible environment. We are going to leverage it to run our tests before even building the different assets.</p><p>So how does it translate to our previous example ? Just how I think you would do it given the explanation above.</p><!--kg-card-begin: code--><figure class="kg-card kg-code-card"><pre><code class="language-dockerfile">FROM python:3.6 as build
COPY requirements.txt .
RUN pip install -q -r requirements.txt
COPY . .
RUN flake8 &amp;&amp; pytest &amp;&amp; python setup.py -q sdist
...
FROM python:3.6-slim-jessie
COPY --from=build /usr/local/lib/python3.6/site-packages/ /usr/local/lib/python3.6/site-packages/
COPY --from=build /usr/local/bin /usr/local/bin
...</code></pre><figcaption>Dockerfile - Test in the first level before actual image in the final level</figcaption></figure><!--kg-card-end: code--><p>We run the tests just before building the egg. If our tests fail, the build fails too independently of the platform running Docker. You get a report, you can fix before relaunching the artifact building.</p><p><strong><strong>Takeaway</strong></strong>: Build the safer artifacts possible by testing in the build phase</p><h1 id="simplify-ci-and-deployments">Simplify CI and deployments</h1><p>At first, I was storing many types of artifacts. This implied managing multiple credentials over multiple platforms (because I was working with some free online services, no money :) ), setting up an upload type per artifact (docker registry, pypi), building then sending each artifact after each successful CI build. Anyway, this can only be useful if both artifacts are used. This could be the case if I was using several types of deployments (container in development environment, VMs in production) or I was developing an application for a client and agreed on a specific artifact to deliver for example.</p><p>But as I owned the application and planned to use Docker from my laptop to my production environment, it simply did not make sense for me to manage several types of artifacts. And it did sting me several times when I made mistakes and wanted to delete the built versions. Finally, I decided that the only artifact that matters is the Docker image. I am now using a Docker registry as my single artifact repository as this is the only thing I need to deploy whatever environments I deploy on. One artifact, one registry, for simpler CI builds and deployments.</p><p><strong><strong>Takeaway</strong></strong>: This one is rather general but do not build unnecessary artifacts on your road to production, keep your deployments simple.</p><!--kg-card-begin: hr--><hr><!--kg-card-end: hr--><h1 id="the-next-mis-steps">The next (mis)steps</h1><p>All in all, I am really glad I learnt this way. Docker was in fact a right tool to do that. You can learn it as you go and Docker always has something to teach you along the way. I made mistakes, I learnt from them and there is still a lot to learn before being sufficiently proficient. How marvelous.</p><p>The next steps I want to explore: improve security of my containers, deploy a simple and informative monitoring for my containers, play more with container orchestration, see what LinuxKit can bring to the table.</p><p>I will be glad to read your tips in the comments. Here are resources I used:</p><p>Official Docker documentation</p><p><a href="https://docs.docker.com/?source=post_page---------------------------">https://docs.docker.com/?source=post_page---------------------------</a></p><p>Docker training</p><p><a href="https://container.training/?source=post_page---------------------------">//container.training/?source=post_page---------------------------</a></p><p><a href="https://training.play-with-docker.com/ops-landing/?source=post_page---------------------------">//training.play-with-docker.com/ops-landing/?source=post_page---------------------------</a></p><p>Play with Docker, to use Docker without installing it</p><p><a href="https://labs.play-with-docker.com/?source=post_page---------------------------">https://labs.play-with-docker.com/?source=post_page---------------------------</a></p>]]></content:encoded></item></channel></rss>